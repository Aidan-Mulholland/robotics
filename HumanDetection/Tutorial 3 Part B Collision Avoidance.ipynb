{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 3 Part B Collision Avoidance\n",
    "\n",
    "This tutorial will show you how to use a captured depth image to avoid collisions. A basic understanding of Python and NumPy arrays is required to implement image processing techniques. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The depth image provides distance information about the scene. Collision avoidance can be achieved by calculating the shortest distance directly in front of the robot. Since the camera is fixed at the front, we can approximate the robot's frontal area as the central region of the captured image. The following image, with a resolution of 672×376, illustrates the defined central area:\n",
    "\n",
    "![TEST](pixel.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To focus on the central area, we can first set the values of the surrounding area to 0 and then remove them later by considering only the non-zero regions. (Note: When implementing your algorithm in Python, avoid using for loops, as they are very slow.) The following code demonstrates how to set 0 values in an array named depth_image.\n",
    "\n",
    "                depth_image[:94,:] = 0\n",
    "                depth_image[282:,:]=0\n",
    "                depth_image[:,:168]=0\n",
    "                depth_image[:,504:]=0\n",
    "\n",
    "Remember that in Python, the indexing convention for an array follows the format array[row, column]. The coordinates provided above are just an example. In practice, you may need to adjust these values to achieve better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have extracted the central area, we only need to find the minimum value within this region to perform object avoidance. Keep in mind that sensor readings may contain noise. We can easily filter out these noises, along with some distant objects, using the following code:\n",
    "\n",
    "                depth_image[depth_image<100]=0\n",
    "                depth_image[depth_image>1000]=0\n",
    "\n",
    "Normally, we could use a for loop to iterate over each pixel in depth_image and compare its value to a threshold. However, this approach significantly impacts processing speed, as Python is inefficient when handling array operations using loops. To improve performance, it is recommended to use built-in array operations whenever possible.\n",
    "\n",
    "For example, NumPy allows direct comparison between an array and a number using the statement depth_image < 100. This is an element-wise operation that compares each pixel's value to the threshold and returns a Boolean array of True or False values. We can then use this Boolean array to index depth_image and set the True areas to 0.\n",
    "\n",
    "Below is a link to learn more about NumPy operations:\n",
    "https://numpy.org/devdocs/user/absolute_beginners.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the minimum value inside an array can be found using the following code:\n",
    "\n",
    "                depth_image[depth_image!=0].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the code above has a small issue: if all values in depth_image are 0, this command will result in an error. Check the following code to see how to fix this bug. Try running it and explore further improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c01be3f911ea4c0ea21e25be812b3de1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'', format='jpeg', width='30%'), Image(value=b'', format='jpeg', width='30%')), la…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-19 10:49:27 UTC][ZED][INFO] Logging level INFO\n",
      "[2025-02-19 10:49:27 UTC][ZED][INFO] Logging level INFO\n",
      "[2025-02-19 10:49:27 UTC][ZED][INFO] Logging level INFO\n",
      "[2025-02-19 10:49:28 UTC][ZED][INFO] [Init]  Depth mode: ULTRA\n",
      "[2025-02-19 10:49:29 UTC][ZED][INFO] [Init]  Camera successfully opened.\n",
      "[2025-02-19 10:49:29 UTC][ZED][INFO] [Init]  Camera FW version: 1523\n",
      "[2025-02-19 10:49:29 UTC][ZED][INFO] [Init]  Video mode: VGA@100\n",
      "[2025-02-19 10:49:29 UTC][ZED][INFO] [Init]  Serial Number: S/N 32322913\n"
     ]
    }
   ],
   "source": [
    "#create two widgets for the displaying of the image\n",
    "import ipywidgets.widgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "import sys\n",
    "\n",
    "#create widgets for the displaying of the image\n",
    "display_color = widgets.Image(format='jpeg', width='30%') #determine the width of the color image\n",
    "display_depth = widgets.Image(format='jpeg', width='30%')  #determine the width of the depth image\n",
    "layout=widgets.Layout(width='100%')\n",
    "\n",
    "sidebyside = widgets.HBox([display_color, display_depth],layout=layout) #horizontal \n",
    "display(sidebyside) #display the widget\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pyzed.sl as sl\n",
    "import math\n",
    "import numpy as np\n",
    "import sys\n",
    "import math\n",
    "\n",
    "#multi-threading is used to capture the image in real time performance\n",
    "import threading\n",
    "\n",
    "import time\n",
    "import motors\n",
    "\n",
    "#initialize the Robot class\n",
    "robot = motors.MotorsYukon(mecanum=False)\n",
    "\n",
    "class Camera():\n",
    "    def __init__(self):\n",
    "        super(Camera, self).__init__()\n",
    "\n",
    "        self.zed = sl.Camera()\n",
    "        # Create a InitParameters object and set configuration parameters\n",
    "        init_params = sl.InitParameters()\n",
    "        init_params.camera_resolution = sl.RESOLUTION.VGA #VGA(672*376), HD720(1280*720), HD1080 (1920*1080) or ...\n",
    "        init_params.depth_mode = sl.DEPTH_MODE.ULTRA  # Use ULTRA depth mode\n",
    "        init_params.coordinate_units = sl.UNIT.MILLIMETER  # Use meter units (for depth measurements)\n",
    "\n",
    "        # Open the camera\n",
    "        status = self.zed.open(init_params)\n",
    "        if status != sl.ERROR_CODE.SUCCESS: #Ensure the camera has opened succesfully\n",
    "            print(\"Camera Open : \"+repr(status)+\". Exit program.\")\n",
    "            self.zed.close()\n",
    "            exit(1)\n",
    "\n",
    "         # Create and set RuntimeParameters after opening the camera\n",
    "        self.runtime = sl.RuntimeParameters()\n",
    "\n",
    "        #flag to control the thread\n",
    "        self.thread_runnning_flag = False\n",
    "\n",
    "        # Get the height and width\n",
    "        camera_info = self.zed.get_camera_information()\n",
    "        self.width = camera_info.camera_configuration.resolution.width\n",
    "        self.height = camera_info.camera_configuration.resolution.height\n",
    "        self.image = sl.Mat(self.width,self.height,sl.MAT_TYPE.U8_C4, sl.MEM.CPU)\n",
    "        self.depth = sl.Mat(self.width,self.height,sl.MAT_TYPE.F32_C1, sl.MEM.CPU)\n",
    "        self.point_cloud = sl.Mat(self.width,self.height,sl.MAT_TYPE.F32_C4, sl.MEM.CPU) \n",
    "\n",
    "    def _capture_frames(self):\n",
    "\n",
    "        while(self.thread_runnning_flag==True): #continue until the thread_runnning_flag is set to be False\n",
    "           \n",
    "            if self.zed.grab(self.runtime) == sl.ERROR_CODE.SUCCESS:\n",
    "                \n",
    "                # Retrieve Left image\n",
    "                self.zed.retrieve_image(self.image, sl.VIEW.LEFT)\n",
    "                # Retrieve depth map. Depth is aligned on the left image\n",
    "                self.zed.retrieve_measure(self.depth, sl.MEASURE.DEPTH)\n",
    "    \n",
    "                self.color_value = self.image.get_data()\n",
    "                #When displaying the images, the robot sends data to the computers. If the images are in very high resolution, it may cause network issues. Since the images are for display purposes only, please use a lower resolution if possible. Thank you.\n",
    "                scale = 0.1\n",
    "                resized_image = cv2.resize(self.color_value, None, fx=scale, fy=scale, interpolation=cv2.INTER_AREA)\n",
    "                cv2.circle(resized_image, (int(self.width*scale//2),int(self.height*scale//2)), 1, (0, 255, 0))\n",
    "                display_color.value = bgr8_to_jpeg(resized_image)\n",
    "                \n",
    "                self.depth_image = np.asanyarray(self.depth.get_data())\n",
    "\n",
    "                depth_image_test = self.depth_image.copy()  #copy the depth image\n",
    "                depth_image_test = np.nan_to_num(depth_image_test, nan=0.0).astype(np.float32)  # Change NaN value to 0\n",
    "                depth_image_test[:94,:] = 0\n",
    "                depth_image_test[282:,:]=0\n",
    "                depth_image_test[:,:168]=0\n",
    "                depth_image_test[:,504:]=0\n",
    "\n",
    "                depth_image_test[depth_image_test<200]=0\n",
    "                depth_image_test[depth_image_test>1000]=1000\n",
    "                depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image_test, alpha=0.03), cv2.COLORMAP_JET)\n",
    "                if(depth_image_test.max() == 0):\n",
    "                    robot.backward(1) # Turn back\n",
    "                    cv2.putText(depth_colormap, 'warning!!!', (336,188), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "                else:\n",
    "                    left_side=depth_image_test[:, :336]\n",
    "                    left_side_distance = left_side[left_side!=0].min()\n",
    "                    right_side=depth_image_test[:, 337:]\n",
    "                    right_side_distance = right_side[right_side!=0].min()\n",
    "\n",
    "                    if (left_side_distance < right_side_distance and left_side_distance < 400):\n",
    "                        robot.right(0.7)\n",
    "                        cv2.putText(depth_colormap, 'warning!!!', (336,188), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "                    elif(right_side_distance < left_side_distance and right_side_distance < 400):\n",
    "                        robot.left(0.7) #turn left at full speed\n",
    "                        cv2.putText(depth_colormap, 'warning!!!', (336,188), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "                    else:\n",
    "                        robot.forward(0.4) #move forward at half speed\n",
    "                    \n",
    "                resized_depth_colormap=cv2.resize(depth_colormap, None, fx=scale, fy=scale, interpolation=cv2.INTER_AREA)\n",
    "                display_depth.value = bgr8_to_jpeg(resized_depth_colormap)\n",
    "                \n",
    "    def start(self): #start the data capture thread\n",
    "        if self.thread_runnning_flag == False: #only process if no thread is running yet\n",
    "            self.thread_runnning_flag=True #flag to control the operation of the _capture_frames function\n",
    "            self.thread = threading.Thread(target=self._capture_frames) #link thread with the function\n",
    "            self.thread.start() #start the thread\n",
    "\n",
    "    def stop(self): #stop the data capture thread\n",
    "        if self.thread_runnning_flag == True:\n",
    "            self.thread_runnning_flag = False #exit the while loop in the _capture_frames\n",
    "            self.thread.join() #wait the exiting of the thread\n",
    "            self.zed.close()\n",
    "\n",
    "def bgr8_to_jpeg(value):#convert numpy array to jpeg coded data for displaying \n",
    "    return bytes(cv2.imencode('.jpg',value)[1])\n",
    "\n",
    "#create a camera object\n",
    "camera = Camera()\n",
    "camera.start() # start capturing the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.stop()\n",
    "robot.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tasks**\n",
    "\n",
    "1.\tUnderstand how collision avoidance can be achieved.\n",
    "\n",
    "2.\tDifferent robots have different viewing angles, so the predefined area might not work best for your robot. Please adjust the area to see if the performance can be improved.\n",
    "\n",
    "3.\tPlease adjust the distance threshold and center region to achieve the best performance  \n",
    "\n",
    "4.\tIn the demo code, the robot will always turn left after detecting the collisions. Are there better strategies to handle this?\n",
    "\n",
    "Optional: Check whether the program can run while simultaneously recording data.\n",
    "\n",
    "Note: Please remember to delete your code on the robot once the experiment is finished."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
