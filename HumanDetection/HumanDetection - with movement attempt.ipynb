{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 5 -- Human Detection \n",
    "In this tutorial, we will use multithreading to capture both color and depth images. We will then employ the YOLOv11 neural network architecture for human detection. To ensure real-time performance, we will utilize the TensorRT deep learning framework. For more details, visit the following website: https://docs.ultralytics.com/guides/nvidia-jetson/#convert-model-to-tensorrt-and-run-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have uploaded two YOLO TensorRT models to the LEARN page: the YOLO11L FP16 version and the YOLO11N FP32 version. \n",
    "# The first model, 'YOLO11n.engine', runs at a faster speed but has limited detection accuracy. \n",
    "# The second model, 'yolo11l_half.engine', is the FP16 version, as indicated by 'half' in its name.\n",
    "# If you need other versions, please refer to the following link:\n",
    "# https://docs.ultralytics.com/modes/export/#arguments\n",
    "\n",
    "# Below is the code I used to convert the YOLO11L FP16 model\n",
    "# from ultralytics import YOLO\n",
    "# model = YOLO(\"yolo11l.pt\")  \n",
    "# model.export(format=\"engine\",half=True)  # FP16\n",
    "# Note: Generating the 'yolo11l.engine' file may take a long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.stop()\n",
    "robot.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the ZED2i Camera system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n",
      "[2025-03-12 12:53:28 UTC][ZED][INFO] Logging level INFO\n",
      "[2025-03-12 12:53:28 UTC][ZED][INFO] Logging level INFO\n",
      "[2025-03-12 12:53:28 UTC][ZED][INFO] Logging level INFO\n",
      "[2025-03-12 12:53:29 UTC][ZED][INFO] [Init]  Depth mode: ULTRA\n",
      "[2025-03-12 12:53:30 UTC][ZED][INFO] [Init]  Camera successfully opened.\n",
      "[2025-03-12 12:53:30 UTC][ZED][INFO] [Init]  Camera FW version: 1523\n",
      "[2025-03-12 12:53:30 UTC][ZED][INFO] [Init]  Video mode: VGA@100\n",
      "[2025-03-12 12:53:30 UTC][ZED][INFO] [Init]  Serial Number: S/N 35566780\n",
      "Loading yolo11l_half.engine for TensorRT inference...\n",
      "Average Depth: 492.2709655761719 mmm"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Depth: nan mm07678222656 mm"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4135/1103006904.py:119: RuntimeWarning: Mean of empty slice\n",
      "  average_depth = np.nanmean(trimmed_depth_image_cleaned)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Depth: 1257.39453125 mm5 mmm"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4135/1103006904.py:119: RuntimeWarning: Mean of empty slice\n",
      "  average_depth = np.nanmean(trimmed_depth_image_cleaned)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Depth: nan mm23449707031 mmm"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Depth: 2231.59375 mm0625 mm"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4135/1103006904.py:119: RuntimeWarning: Mean of empty slice\n",
      "  average_depth = np.nanmean(trimmed_depth_image_cleaned)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Depth: nan mmm271484375 mmmm"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4135/1103006904.py:119: RuntimeWarning: Mean of empty slice\n",
      "  average_depth = np.nanmean(trimmed_depth_image_cleaned)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Depth: nan mm919921875 mmmmm"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4135/3393630605.py:119: RuntimeWarning: Mean of empty slice\n",
      "  average_depth = np.nanmean(trimmed_depth_image_cleaned)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Depth: nan mm"
     ]
    }
   ],
   "source": [
    "# You will need to load the YOLO model if you skip the first code block.\n",
    "from ultralytics import YOLO\n",
    "model = YOLO(\"yolo11l_half.engine\")\n",
    "\n",
    "#Start the camera system\n",
    "import traitlets\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pyzed.sl as sl\n",
    "import math\n",
    "import numpy as np\n",
    "import sys\n",
    "import math\n",
    "import threading\n",
    "from traitlets.config.configurable import SingletonConfigurable\n",
    "\n",
    "# Define a Camera class that inherits from SingletonConfigurable\n",
    "class Camera(SingletonConfigurable):\n",
    "    color_value = traitlets.Any() # monitor the color_value variable\n",
    "    def __init__(self):\n",
    "        super(Camera, self).__init__()\n",
    "\n",
    "        self.zed = sl.Camera()\n",
    "        # Create a InitParameters object and set configuration parameters\n",
    "        init_params = sl.InitParameters()\n",
    "        init_params.camera_resolution = sl.RESOLUTION.VGA #VGA(672*376), HD720(1280*720), HD1080 (1920*1080) or ...\n",
    "        init_params.depth_mode = sl.DEPTH_MODE.ULTRA  # Use ULTRA depth mode\n",
    "        init_params.coordinate_units = sl.UNIT.MILLIMETER  # Use meter units (for depth measurements)\n",
    "\n",
    "        # Open the camera\n",
    "        status = self.zed.open(init_params)\n",
    "        if status != sl.ERROR_CODE.SUCCESS: #Ensure the camera has opened succesfully\n",
    "            print(\"Camera Open : \"+repr(status)+\". Exit program.\")\n",
    "            self.zed.close()\n",
    "            exit(1)\n",
    "\n",
    "         # Create and set RuntimeParameters after opening the camera\n",
    "        self.runtime = sl.RuntimeParameters()\n",
    "\n",
    "        #flag to control the thread\n",
    "        self.thread_runnning_flag = False\n",
    "\n",
    "        # Get the height and width\n",
    "        camera_info = self.zed.get_camera_information()\n",
    "        self.width = camera_info.camera_configuration.resolution.width\n",
    "        self.height = camera_info.camera_configuration.resolution.height\n",
    "        self.image = sl.Mat(self.width,self.height,sl.MAT_TYPE.U8_C4, sl.MEM.CPU)\n",
    "        self.depth = sl.Mat(self.width,self.height,sl.MAT_TYPE.F32_C1, sl.MEM.CPU)\n",
    "        self.point_cloud = sl.Mat(self.width,self.height,sl.MAT_TYPE.F32_C4, sl.MEM.CPU) \n",
    "\n",
    "    def _capture_frames(self): #For data capturing only\n",
    "\n",
    "        while(self.thread_runnning_flag==True): #continue until the thread_runnning_flag is set to be False\n",
    "            if self.zed.grab(self.runtime) == sl.ERROR_CODE.SUCCESS:\n",
    "                \n",
    "                # Retrieve Left image\n",
    "                self.zed.retrieve_image(self.image, sl.VIEW.LEFT)\n",
    "                # Retrieve depth map. Depth is aligned on the left image\n",
    "                self.zed.retrieve_measure(self.depth, sl.MEASURE.DEPTH)\n",
    "    \n",
    "                self.color_value_BGRA = self.image.get_data()\n",
    "                self.color_value= cv2.cvtColor(self.color_value_BGRA, cv2.COLOR_BGRA2BGR)\n",
    "                self.depth_image = np.asanyarray(self.depth.get_data())   \n",
    "                \n",
    "    def start(self): #start the data capture thread\n",
    "        if self.thread_runnning_flag == False: #only process if no thread is running yet\n",
    "            self.thread_runnning_flag=True #flag to control the operation of the _capture_frames function\n",
    "            self.thread = threading.Thread(target=self._capture_frames) #link thread with the function\n",
    "            self.thread.start() #start the thread\n",
    "\n",
    "    def stop(self): #stop the data capture thread\n",
    "        if self.thread_runnning_flag == True:\n",
    "            self.thread_runnning_flag = False #exit the while loop in the _capture_frames\n",
    "            self.thread.join() #wait the exiting of the thread  \n",
    "            self.zed.close()\n",
    "\n",
    "def bgr8_to_jpeg(value):#convert numpy array to jpeg coded data for displaying \n",
    "    return bytes(cv2.imencode('.jpg',value)[1])\n",
    "\n",
    "#create a camera object\n",
    "camera = Camera()\n",
    "camera.start() # start capturing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform object detection on live video data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cae23554b26449e0a3f35b5f99b3a567",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'', format='jpeg', width='45%'), Image(value=b'', format='jpeg', width='45%')), la…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2\n",
    "import ipywidgets.widgets as widgets\n",
    "from IPython.display import display\n",
    "import collections\n",
    "import motors\n",
    "robot = motors.MotorsYukon(mecanum=False)\n",
    "\n",
    "#create widgets for the displaying of the image\n",
    "display_color = widgets.Image(format='jpeg', width='45%') \n",
    "display_depth = widgets.Image(format='jpeg', width='45%')  \n",
    "layout=widgets.Layout(width='100%')\n",
    "\n",
    "sidebyside = widgets.HBox([display_color, display_depth],layout=layout) #horizontal \n",
    "display(sidebyside) #display the widget\n",
    "\n",
    "tracked_bbox = None\n",
    "x_center_t = None\n",
    "y_center_t = None\n",
    "\n",
    "def moving_average(bbox_history):\n",
    "    # Compute the average of all the bounding box coordinates\n",
    "    avg_bbox = np.mean(bbox_history, axis=0)\n",
    "    return avg_bbox\n",
    "\n",
    "#callback function, invoked when traitlets detects the changing of the color image\n",
    "def func(change):\n",
    "    global tracked_bbox\n",
    "    global x_center_t\n",
    "    global y_center_t\n",
    "\n",
    "    frame = change['new']\n",
    "    results = model(frame,verbose=False)\n",
    "\n",
    "    trim=[0,0,0,0]\n",
    "\n",
    "    conf_threshold = 0.6\n",
    "    \n",
    "    for result in results: #the input image is one so only one result avaialbe\n",
    "\n",
    "        chosen_box = None\n",
    "        \n",
    "        for i in range (len(result.boxes.cls)):\n",
    "            if(result.boxes.cls[i] == 0):  #human subject\n",
    "                if (result.boxes.conf[i] > conf_threshold): #confident its a human\n",
    "                    bbox = result.boxes.xyxy[i]\n",
    "                    cv2.rectangle(frame, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), (255, 0, 0), 2)   \n",
    "\n",
    "                    if tracked_bbox is None:\n",
    "                        chosen_box = bbox\n",
    "                        break;\n",
    "                    \n",
    "                    if chosen_box is None:\n",
    "                        chosen_box = bbox\n",
    "                    else:\n",
    "                        x_center_c = (chosen_box[0] + chosen_box[2]) / 2\n",
    "                        y_center_c = (chosen_box[1] + chosen_box[3]) / 2\n",
    "                        x_center = (bbox[0] + bbox[2]) / 2\n",
    "                        y_center = (bbox[1] + bbox[3]) / 2\n",
    "\n",
    "                        # difference\n",
    "                        distance_new = math.sqrt((x_center - x_center_t)**2 + (y_center - y_center_t)**2)\n",
    "                        distance_chosen = math.sqrt((x_center_c - x_center_t)**2 + (y_center_c - y_center_t)**2)\n",
    "\n",
    "                        if distance_new < distance_chosen:\n",
    "                            chosen_box = bbox\n",
    "                        \n",
    "\n",
    "        if chosen_box is None:\n",
    "            break;\n",
    "        tracked_bbox = chosen_box\n",
    "        trim = [int(tracked_bbox[1]), int(tracked_bbox[3]), int(tracked_bbox[0]), int(tracked_bbox[2])]\n",
    "        trim[0] = int(tracked_bbox[1])\n",
    "        trim[1] = int(tracked_bbox[3])\n",
    "        trim[2] = int(tracked_bbox[0])\n",
    "        trim[3] = int(tracked_bbox[2])\n",
    "\n",
    "        x_center_t = (tracked_bbox[0] + tracked_bbox[2]) / 2\n",
    "        y_center_t = (tracked_bbox[1] + tracked_bbox[3]) / 2\n",
    "        \n",
    "        # Draw a circle at the center of the bounding box\n",
    "        radius = 10  # You can adjust the radius as needed\n",
    "        color = (0, 255, 0)  # Circle color (Green in BGR format)\n",
    "        thickness = 2  # Circle border thickness\n",
    "        cv2.circle(frame, (int(x_center_t), int(y_center_t)), radius, color, thickness)\n",
    "        \n",
    "    #Scaling is necessary for real-time data display.\n",
    "    scale = 0.1 \n",
    "\n",
    "    # Resize the frame for display\n",
    "    resized_image = cv2.resize(frame, None, fx=scale, fy=scale, interpolation=cv2.INTER_AREA)\n",
    "    display_color.value = bgr8_to_jpeg(resized_image)\n",
    "    \n",
    "    # Process depth image (apply colormap)\n",
    "    depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(camera.depth_image , alpha=0.03), cv2.COLORMAP_JET)\n",
    "    \n",
    "    # Trim the depth image based on tracked bounding box coordinates\n",
    "    depth_colormap[:trim[0], :] = 0\n",
    "    depth_colormap[trim[1]:, :] = 0\n",
    "    depth_colormap[:, :trim[2]] = 0\n",
    "    depth_colormap[:, trim[3]:] = 0\n",
    "\n",
    "\n",
    "    # Resize the depth colormap for display\n",
    "    resized_depth_colormap = cv2.resize(depth_colormap, None, fx=scale, fy=scale, interpolation=cv2.INTER_AREA)\n",
    "    display_depth.value = bgr8_to_jpeg(resized_depth_colormap)\n",
    "\n",
    "    if x_center_t is not None and y_center_t is not None:\n",
    "        # Now calculate the average depth of the entire depth image (before trimming and resizing)\n",
    "        # Trim depth image to the region of interest\n",
    "        trimmed_depth_image = camera.depth_image[trim[0]:trim[1], trim[2]:trim[3]]\n",
    "        \n",
    "        # Remove NaN and invalid depth values (if necessary)\n",
    "        trimmed_depth_image_cleaned = np.nan_to_num(trimmed_depth_image, nan=0.0).astype(np.float32)\n",
    "        \n",
    "        # Optionally, replace zero values with NaN (if they represent no depth or invalid data)\n",
    "        trimmed_depth_image_cleaned[trimmed_depth_image_cleaned == 0] = np.nan\n",
    "        \n",
    "        # Calculate the average depth from the trimmed depth image\n",
    "        average_depth = np.nanmean(trimmed_depth_image_cleaned)\n",
    "        \n",
    "        # Print the average depth\n",
    "        print(f\"Average Depth: {average_depth} mm\", end='\\r')\n",
    "    \n",
    "        if x_center_t > 362:\n",
    "            robot.right(0.2)\n",
    "        elif x_center_t < 310:\n",
    "            robot.left(0.2)\n",
    "        elif average_depth > 1800: # TODO cut off the closest and highest values from the image first e.g. >2000 and <200\n",
    "            robot.forward(0.5)\n",
    "        else:\n",
    "            robot.stop()\n",
    "    \n",
    "    \n",
    "\n",
    "    # scale = 0.1\n",
    "    # resized_image = cv2.resize(camera.color_value, None, fx=scale, fy=scale, interpolation=cv2.INTER_AREA)\n",
    "    # display_color.value = bgr8_to_jpeg(resized_image)\n",
    "    \n",
    "    # camera.depth_image = np.asanyarray(camera.depth.get_data())\n",
    "\n",
    "    # depth_image_test = camera.depth_image.copy()  #copy the depth image\n",
    "    # depth_image_test = np.nan_to_num(depth_image_test, nan=0.0).astype(np.float32)  # Change NaN value to 0\n",
    "    # depth_image_test[:trim[0],:] =0\n",
    "    # depth_image_test[trim[1]:,:]=0\n",
    "    # depth_image_test[:,:trim[2]]=0\n",
    "    # depth_image_test[:,trim[3]:]=0\n",
    "\n",
    "    # depth_image_test[depth_image_test<200]=0\n",
    "    # depth_image_test[depth_image_test>1000]=1000\n",
    "    # depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image_test, alpha=0.03), cv2.COLORMAP_JET)\n",
    "    # # if(depth_image_test.max() == 0):\n",
    "    # #     robot.backward(1) # Turn back\n",
    "    # #     cv2.putText(depth_colormap, 'warning!!!', (336,188), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "    # # else:\n",
    "    # #     left_side=depth_image_test[:, :336]\n",
    "    # #     left_side_distance = left_side[left_side!=0].min()\n",
    "    # #     right_side=depth_image_test[:, 337:]\n",
    "    # #     right_side_distance = right_side[right_side!=0].min()\n",
    "\n",
    "    # #     if (left_side_distance < right_side_distance and left_side_distance < 400):\n",
    "    # #         robot.right(0.7)\n",
    "    # #         cv2.putText(depth_colormap, 'warning!!!', (336,188), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "    # #     elif(right_side_distance < left_side_distance and right_side_distance < 400):\n",
    "    # #         robot.left(0.7) #turn left at full speed\n",
    "    # #         cv2.putText(depth_colormap, 'warning!!!', (336,188), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "    # #     else:\n",
    "    # #         robot.forward(0.4) #move forward at half speed\n",
    "        \n",
    "    # resized_depth_colormap=cv2.resize(depth_colormap, None, fx=scale, fy=scale, interpolation=cv2.INTER_AREA)\n",
    "    # display_depth.value = bgr8_to_jpeg(resized_depth_colormap)\n",
    "    \n",
    "camera.observe(func, names=['color_value'])\n",
    "# camera.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "1. Please try to calculate the distance between the detected human and the robot using the depth image. (Note: You can refer to Tutorial 2 to obtain the depth information for a specific point.) \n",
    "\n",
    "2. Please try to add a collision avoidance function to this program to protect the robot.  \n",
    "\n",
    "3. Think about how to control the robot so that it moves towards the detected human. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
