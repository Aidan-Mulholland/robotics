{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human Detection Movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- IMPORTING LIBRARIES ---\n",
    "from ultralytics import YOLO\n",
    "import ipywidgets.widgets as widgets\n",
    "from IPython.display import display\n",
    "import traitlets\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pyzed.sl as sl\n",
    "import math\n",
    "import numpy as np\n",
    "import sys\n",
    "import math\n",
    "import threading\n",
    "from traitlets.config.configurable import SingletonConfigurable\n",
    "\n",
    "import motors # ADDED LIBRARY\n",
    "\n",
    "def bgr8_to_jpeg(value):\n",
    "    return bytes(cv2.imencode('.jpg',value)[1])\n",
    "\n",
    "# --- Load the YOLO model ---\n",
    "model = YOLO(\"yolo11l_half.engine\")\n",
    "# model = YOLO(\"yolo11n.engine\")\n",
    "\n",
    "# Initialize Robot Class\n",
    "robot = motors.MotorsYukon(mecanum=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-12 11:03:42 UTC][ZED][INFO] Logging level INFO\n",
      "[2025-03-12 11:03:42 UTC][ZED][INFO] Logging level INFO\n",
      "[2025-03-12 11:03:42 UTC][ZED][INFO] Logging level INFO\n",
      "[2025-03-12 11:03:43 UTC][ZED][INFO] [Init]  Depth mode: ULTRA\n",
      "[2025-03-12 11:03:44 UTC][ZED][INFO] [Init]  Camera successfully opened.\n",
      "[2025-03-12 11:03:44 UTC][ZED][INFO] [Init]  Camera FW version: 1523\n",
      "[2025-03-12 11:03:44 UTC][ZED][INFO] [Init]  Video mode: VGA@100\n",
      "[2025-03-12 11:03:44 UTC][ZED][INFO] [Init]  Serial Number: S/N 32565960\n",
      "[2025-03-12 11:03:44 UTC][ZED][WARNING] [Init]  Self-calibration failed. Point the camera towards a more textured and brighter area. Avoid objects closer than 1 meter (Error code: 0x01) \n",
      "Loading yolo11l_half.engine for TensorRT inference...\n"
     ]
    }
   ],
   "source": [
    "class Camera(SingletonConfigurable):\n",
    "    color_value = traitlets.Any()\n",
    "    def __init__(self):\n",
    "        super(Camera, self).__init__()\n",
    "\n",
    "        self.zed = sl.Camera()\n",
    "\n",
    "        init_params = sl.InitParameters()\n",
    "        init_params.camera_resolution = sl.RESOLUTION.VGA #VGA(672*376), HD720(1280*720), HD1080 (1920*1080) or ...\n",
    "        init_params.depth_mode = sl.DEPTH_MODE.ULTRA  # Use ULTRA depth mode\n",
    "        init_params.coordinate_units = sl.UNIT.MILLIMETER  # Use meter units (for depth measurements)\n",
    "\n",
    "        status = self.zed.open(init_params)\n",
    "        if status != sl.ERROR_CODE.SUCCESS:\n",
    "            print(\"Camera Open : \"+repr(status)+\". Exit program.\")\n",
    "            self.zed.close()\n",
    "            exit(1)\n",
    "\n",
    "        self.runtime = sl.RuntimeParameters()\n",
    "\n",
    "        self.thread_runnning_flag = False\n",
    "\n",
    "        camera_info = self.zed.get_camera_information()\n",
    "        self.width = camera_info.camera_configuration.resolution.width\n",
    "        self.height = camera_info.camera_configuration.resolution.height\n",
    "        self.image = sl.Mat(self.width,self.height,sl.MAT_TYPE.U8_C4, sl.MEM.CPU)\n",
    "        self.depth = sl.Mat(self.width,self.height,sl.MAT_TYPE.F32_C1, sl.MEM.CPU)\n",
    "        self.point_cloud = sl.Mat(self.width,self.height,sl.MAT_TYPE.F32_C4, sl.MEM.CPU) \n",
    "\n",
    "    def _capture_frames(self):\n",
    "\n",
    "        while(self.thread_runnning_flag==True):\n",
    "            if self.zed.grab(self.runtime) == sl.ERROR_CODE.SUCCESS:\n",
    "                \n",
    "                self.zed.retrieve_image(self.image, sl.VIEW.LEFT)\n",
    "                self.zed.retrieve_measure(self.depth, sl.MEASURE.DEPTH)\n",
    "    \n",
    "                self.color_value_BGRA = self.image.get_data()\n",
    "                self.color_value= cv2.cvtColor(self.color_value_BGRA, cv2.COLOR_BGRA2BGR)\n",
    "                self.depth_image = np.asanyarray(self.depth.get_data()) \n",
    "\n",
    "                \"\"\"\n",
    "                depth_image_test = self.depth_image.copy()  #copy the depth image\n",
    "                depth_image_test = np.nan_to_num(depth_image_test, nan=0.0).astype(np.float32)\n",
    "                depth_image_test[:94,:] = 0\n",
    "                depth_image_test[282:,:]=0\n",
    "                depth_image_test[:,:168]=0\n",
    "                depth_image_test[:,504:]=0\n",
    "\n",
    "                depth_image_test[depth_image_test<100]=0\n",
    "                depth_image_test[depth_image_test>1000]=0\n",
    "\n",
    "                distance = depth_image_test[depth_image_test!=0].min()\n",
    "                \"\"\"\n",
    "                \n",
    "    def start(self):\n",
    "        if self.thread_runnning_flag == False:\n",
    "            self.thread_runnning_flag=True\n",
    "            self.thread = threading.Thread(target=self._capture_frames)\n",
    "            self.thread.start()\n",
    "\n",
    "    def stop(self):\n",
    "        if self.thread_runnning_flag == True:\n",
    "            self.thread_runnning_flag = False\n",
    "            self.thread.join()    \n",
    "            \n",
    "camera = Camera()\n",
    "camera.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c147dbf6855f4fe39b5f217918fc85c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'', format='jpeg', width='45%'), Image(value=b'', format='jpeg', width='45%')), laâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03/12/2025-11:03:45] [TRT] [I] Loaded engine size: 52 MiB\n",
      "[03/12/2025-11:03:45] [TRT] [W] Using an engine plan file across different models of devices is not recommended and is likely to affect performance or even cause errors.\n",
      "[03/12/2025-11:03:45] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +1, GPU +36, now: CPU 1, GPU 84 (MiB)\n"
     ]
    }
   ],
   "source": [
    "# --- DISPLAY WIDGETS ---\n",
    "display_color = widgets.Image(format='jpeg', width='45%') \n",
    "display_depth = widgets.Image(format='jpeg', width='45%')  \n",
    "layout=widgets.Layout(width='100%')\n",
    "sidebyside = widgets.HBox([display_color, display_depth],layout=layout) \n",
    "display(sidebyside)\n",
    "\n",
    "\n",
    "def func(change):\n",
    "\n",
    "    frame = change['new']\n",
    "    # t1 = cv2.getTickCount()\n",
    "    results = model(frame,verbose=False)\n",
    "    # total_time = (cv2.getTickCount() - t1) / cv2.getTickFrequency()\n",
    "    # print('total_time ',total_time)\n",
    "\n",
    "    # --- COLLISION ALGORITHM [Might need to play around with location]  ---\n",
    "    \"\"\"\n",
    "    depth_image = camera.depth_image\n",
    "    if depth_image is None or depth_image.size == 0:\n",
    "        print(\"Invalid depth image\")\n",
    "        return\n",
    "    \n",
    "    # Clean up the depth image (remove NaN and inf values)\n",
    "    depth_image_clean = np.nan_to_num(depth_image, nan=0.0)  # Replace NaN values with 0.0\n",
    "    depth_image_clean[depth_image_clean < 100] = 0  # Filter out close objects (under 100mm)\n",
    "    depth_image_clean[depth_image_clean > 1000] = 0  # Filter out far objects (over 1000mm)\n",
    "    \n",
    "    # Get the minimum valid depth in the frame (ignoring zeroes)\n",
    "    valid_depth_pixels = depth_image_clean[depth_image_clean != 0]\n",
    "    \n",
    "    if valid_depth_pixels.size == 0:\n",
    "        print(\"No valid depth pixels detected\")\n",
    "        return\n",
    "\n",
    "    distance = valid_depth_pixels.min()  # Get the minimum valid distance\n",
    "    \"\"\"\n",
    "    \n",
    "    conf_threshold = 0.8\n",
    "    frame_center = frame.shape[1] // 2 # GET HORIZONTAL CENTER OF FRAME\n",
    "    if len(results) == 0:\n",
    "        robot.stop()\n",
    "    for result in results:\n",
    "        if 0 in result.boxes.cls:\n",
    "            for i in range (len(result.boxes.cls)):\n",
    "                if(result.boxes.cls[i] == 0):\n",
    "                    if (result.boxes.conf[i] > conf_threshold):\n",
    "                        bbox = result.boxes.xyxy[i]\n",
    "                        cv2.rectangle(frame, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), (255, 0, 0), 2)\n",
    "                        \n",
    "                        # -- MOVEMENT AFTER DETECTING OBJECT ---     \n",
    "                        # --- MOVEMENT WITH CENTER READJUSTMENT ---\n",
    "                        obj_center = int((bbox[0] + bbox[2]) / 2) # GETS OBJECT CENTER\n",
    "                        cv2.circle(frame, (obj_center, int((bbox[1] + bbox[3]) / 2)), 5, (0, 255, 0), -1)\n",
    "                        \n",
    "                        if abs(obj_center - frame_center) > 50:\n",
    "                            if obj_center < frame_center:\n",
    "                                robot.left(0.3)\n",
    "                            else:\n",
    "                                robot.right(0.3)\n",
    "                        else:\n",
    "                            robot.forward(0.5)   \n",
    "        else:\n",
    "            robot.stop()\n",
    "                    \n",
    "    scale = 0.1 \n",
    "    resized_image = cv2.resize(frame, None, fx=scale, fy=scale, interpolation=cv2.INTER_AREA)\n",
    "    display_color.value = bgr8_to_jpeg(resized_image)\n",
    "\n",
    "    depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(camera.depth_image , alpha=0.03), cv2.COLORMAP_JET)\n",
    "    resized_depth_colormap=cv2.resize(depth_colormap, None, fx=scale, fy=scale, interpolation=cv2.INTER_AREA)\n",
    "    display_depth.value = bgr8_to_jpeg(resized_depth_colormap)\n",
    "\n",
    "    # --- ADDED CENTER CIRCLE ---\n",
    "    cv2.circle(resized_image, (int(camera.width*scale//2),int(camera.height*scale//2)), 1, (0, 255, 0))\n",
    "    # --- May need to place center circle code in _capture_frames function ---\n",
    "\n",
    "\n",
    "camera.observe(func, names=['color_value'])\n",
    "# camera.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.stop()\n",
    "robot.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
