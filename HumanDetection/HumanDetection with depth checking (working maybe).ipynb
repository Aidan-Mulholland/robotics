{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.stop()\n",
    "robot.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the ZED2i Camera system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbcda8df1e5d4148bcd1522b4a83cf7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'', format='jpeg', width='45%'), Image(value=b'', format='jpeg', width='45%')), la…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-30 09:54:00 UTC][ZED][INFO] Logging level INFO\n",
      "[2025-04-30 09:54:00 UTC][ZED][INFO] Logging level INFO\n",
      "[2025-04-30 09:54:00 UTC][ZED][INFO] Logging level INFO\n",
      "[2025-04-30 09:54:00 UTC][ZED][INFO] [Init]  Depth mode: ULTRA\n",
      "[2025-04-30 09:54:01 UTC][ZED][INFO] [Init]  Camera successfully opened.\n",
      "[2025-04-30 09:54:01 UTC][ZED][INFO] [Init]  Camera FW version: 1523\n",
      "[2025-04-30 09:54:01 UTC][ZED][INFO] [Init]  Video mode: VGA@100\n",
      "[2025-04-30 09:54:01 UTC][ZED][INFO] [Init]  Serial Number: S/N 33439269\n",
      "[2025-04-30 09:54:01 UTC][ZED][WARNING] [Init]  Self-calibration failed. Point the camera towards a more textured and brighter area. Avoid objects closer than 1 meter (Error code: 0x03) \n",
      "Loading yolo11l_half.engine for TensorRT inference...\n",
      "[04/30/2025-10:54:02] [TRT] [I] Loaded engine size: 52 MiB\n",
      "[04/30/2025-10:54:02] [TRT] [W] Using an engine plan file across different models of devices is not recommended and is likely to affect performance or even cause errors.\n",
      "[04/30/2025-10:54:02] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +1, GPU +36, now: CPU 1, GPU 84 (MiB)\n",
      "NEW TARGET: 542.8740234375 0.11 | right_motor_speed: -0.21\n",
      "Setting new time\n",
      "Speeds: left_motor_speed: -0.12 | right_motor_speed: 0.226"
     ]
    }
   ],
   "source": [
    "# You will need to load the YOLO model if you skip the first code block.\n",
    "from ultralytics import YOLO\n",
    "model = YOLO(\"yolo11l_half.engine\")\n",
    "\n",
    "#Start the camera system\n",
    "import traitlets\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pyzed.sl as sl\n",
    "import math\n",
    "import numpy as np\n",
    "import sys\n",
    "import math\n",
    "import threading\n",
    "import ipywidgets.widgets as widgets\n",
    "from IPython.display import display\n",
    "import collections\n",
    "import motors\n",
    "from traitlets.config.configurable import HasTraits\n",
    "from datetime import timedelta, datetime\n",
    "\n",
    "robot = motors.MotorsYukon(mecanum=False)\n",
    "\n",
    "TARGET_DISTANCE = 1600\n",
    "SCREEN_WIDTH = 672 # VGA width = 672\n",
    "CENTER_X = SCREEN_WIDTH/2  \n",
    "\n",
    "display_color = widgets.Image(format='jpeg', width='45%') \n",
    "display_depth = widgets.Image(format='jpeg', width='45%')  \n",
    "layout=widgets.Layout(width='100%')\n",
    "\n",
    "sidebyside = widgets.HBox([display_color, display_depth],layout=layout) #horizontal \n",
    "display(sidebyside) #display the widget\n",
    "\n",
    "# Define a Camera class that inherits from SingletonConfigurable\n",
    "class Camera(HasTraits):\n",
    "    color_value = traitlets.Any() # monitor the color_value variable\n",
    "    person_pos = traitlets.Any()\n",
    "    def __init__(self):\n",
    "        super(Camera, self).__init__()\n",
    "\n",
    "        self.zed = sl.Camera()\n",
    "        # Create a InitParameters object and set configuration parameters\n",
    "        init_params = sl.InitParameters()\n",
    "        init_params.camera_resolution = sl.RESOLUTION.VGA #VGA(672*376), HD720(1280*720), HD1080 (1920*1080) or ...\n",
    "        init_params.depth_mode = sl.DEPTH_MODE.ULTRA  # Use ULTRA depth mode\n",
    "        init_params.coordinate_units = sl.UNIT.MILLIMETER  # Use meter units (for depth measurements)\n",
    "\n",
    "        # Open the camera\n",
    "        status = self.zed.open(init_params)\n",
    "        if status != sl.ERROR_CODE.SUCCESS: #Ensure the camera has opened succesfully\n",
    "            print(\"Camera Open : \"+repr(status)+\". Exit program.\")\n",
    "            self.zed.close()\n",
    "            exit(1)\n",
    "\n",
    "         # Create and set RuntimeParameters after opening the camera\n",
    "        self.runtime = sl.RuntimeParameters()\n",
    "\n",
    "        #flag to control the thread\n",
    "        self.thread_runnning_flag = False\n",
    "\n",
    "        # Get the height and width\n",
    "        camera_info = self.zed.get_camera_information()\n",
    "        self.width = camera_info.camera_configuration.resolution.width\n",
    "        self.height = camera_info.camera_configuration.resolution.height\n",
    "        self.image = sl.Mat(self.width,self.height,sl.MAT_TYPE.U8_C4, sl.MEM.CPU)\n",
    "        self.depth = sl.Mat(self.width,self.height,sl.MAT_TYPE.F32_C1, sl.MEM.CPU)\n",
    "        self.point_cloud = sl.Mat(self.width,self.height,sl.MAT_TYPE.F32_C4, sl.MEM.CPU)\n",
    "\n",
    "        self.depth_image = np.asanyarray(self.depth.get_data())\n",
    "\n",
    "        self.tracked_bbox = None\n",
    "        self.tracked_depth = 0\n",
    "        self.new_target_time = None\n",
    "        self.x_center_t = None\n",
    "        self.y_center_t = None\n",
    "        \n",
    "\n",
    "    def _capture_frames(self): #For data capturing only\n",
    "\n",
    "        while(self.thread_runnning_flag==True): #continue until the thread_runnning_flag is set to be False\n",
    "            if self.zed.grab(self.runtime) == sl.ERROR_CODE.SUCCESS:\n",
    "                \n",
    "                # Retrieve Left image\n",
    "                self.zed.retrieve_image(self.image, sl.VIEW.LEFT)\n",
    "                # Retrieve depth map. Depth is aligned on the left image\n",
    "                self.zed.retrieve_measure(self.depth, sl.MEASURE.DEPTH)\n",
    "    \n",
    "                self.color_value_BGRA = self.image.get_data()\n",
    "                self.color_value= cv2.cvtColor(self.color_value_BGRA, cv2.COLOR_BGRA2BGR)\n",
    "                self.depth_image = np.asanyarray(self.depth.get_data())\n",
    "                \n",
    "    def start(self): #start the data capture thread\n",
    "        if self.thread_runnning_flag == False: #only process if no thread is running yet\n",
    "            self.thread_runnning_flag=True #flag to control the operation of the _capture_frames function\n",
    "            self.thread = threading.Thread(target=self._capture_frames) #link thread with the function\n",
    "            self.thread.start() #start the thread\n",
    "\n",
    "    def stop(self): #stop the data capture thread\n",
    "        if self.thread_runnning_flag == True:\n",
    "            self.thread_runnning_flag = False #exit the while loop in the _capture_frames\n",
    "            self.thread.join() #wait the exiting of the thread  \n",
    "            self.zed.close()\n",
    "\n",
    "    @traitlets.observe('color_value')\n",
    "    def processFrame(self, change):\n",
    "        \n",
    "        \n",
    "\n",
    "        frame = change['new']\n",
    "        result = model(frame,verbose=False)[0]\n",
    "    \n",
    "        trim=[0,0,0,0]\n",
    "    \n",
    "        conf_threshold = .6\n",
    "        \n",
    "    \n",
    "        chosen_box = None\n",
    "        \n",
    "        for i in range (len(result.boxes.cls)):\n",
    "            if(result.boxes.cls[i] == 0):  #human subject\n",
    "                if (result.boxes.conf[i] > conf_threshold): #confident its a human\n",
    "                    bbox = result.boxes.xyxy[i]\n",
    "                    cv2.rectangle(frame, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), (255, 0, 0), 2)   \n",
    "    \n",
    "                    if self.tracked_bbox is None:\n",
    "                        chosen_box = bbox\n",
    "                        break;\n",
    "                    \n",
    "                    if chosen_box is None:\n",
    "                        chosen_box = bbox\n",
    "                    else:\n",
    "                        x_center_c = (chosen_box[0] + chosen_box[2]) / 2\n",
    "                        y_center_c = (chosen_box[1] + chosen_box[3]) / 2\n",
    "                        x_center = (bbox[0] + bbox[2]) / 2\n",
    "                        y_center = (bbox[1] + bbox[3]) / 2\n",
    "    \n",
    "                        # difference\n",
    "                        distance_new = math.sqrt((x_center - self.x_center_t)**2 + (y_center - self.y_center_t)**2)\n",
    "                        distance_chosen = math.sqrt((x_center_c - self.x_center_t)**2 + (y_center_c - self.y_center_t)**2)\n",
    "    \n",
    "                        if distance_new < distance_chosen:\n",
    "                            chosen_box = bbox\n",
    "                            \n",
    "    \n",
    "        if chosen_box is None:\n",
    "            return;\n",
    "        \n",
    "        trim = [int(chosen_box[1]), int(chosen_box[3]), int(chosen_box[0]), int(chosen_box[2])]\n",
    "        trim[0] = int(chosen_box[1])\n",
    "        trim[1] = int(chosen_box[3])\n",
    "        trim[2] = int(chosen_box[0])\n",
    "        trim[3] = int(chosen_box[2])\n",
    "        trimmed_depth_chosen_image = camera.depth_image[trim[0]:trim[1], trim[2]:trim[3]]\n",
    "\n",
    "\n",
    "        # Remove NaN and invalid depth values (if necessary)\n",
    "        trimmed_depth_chosen_image_cleaned = np.nan_to_num(trimmed_depth_chosen_image, nan=0.0).astype(np.float32)\n",
    "        \n",
    "        # Optionally, replace zero values with NaN (if they represent no depth or invalid data)\n",
    "        trimmed_depth_chosen_image_cleaned[trimmed_depth_chosen_image_cleaned == 0] = np.nan\n",
    "\n",
    "        trimmed_depth_chosen_image_cleaned[trimmed_depth_chosen_image_cleaned<200]=200\n",
    "        trimmed_depth_chosen_image_cleaned[trimmed_depth_chosen_image_cleaned>2000]=2000\n",
    "        \n",
    "        # Calculate the average depth from the trimmed depth image\n",
    "        chosen_depth = np.nanmean(trimmed_depth_chosen_image_cleaned)\n",
    "        if self.tracked_bbox != None:\n",
    "            if chosen_depth is not None or not np.isnan(chosen_depth):\n",
    "                if abs(self.tracked_depth - int(chosen_depth))  > 500:\n",
    "                    print(f\"NEW TARGET: {abs(self.tracked_depth - int(chosen_depth))} \\r\")\n",
    "                    if (self.new_target_time != None and self.new_target_time < datetime.now()):\n",
    "                        print('Passed time')\n",
    "                        self.new_target_time = None\n",
    "                        self.tracked_bbox = chosen_box\n",
    "                    elif (self.new_target_time == None):\n",
    "                        self.new_target_time = datetime.now() + timedelta(seconds=3)\n",
    "                        print('Setting new time')\n",
    "                        pass\n",
    "                    elif (self.new_target_time != None):\n",
    "                        pass\n",
    "                else:\n",
    "                    self.tracked_bbox = chosen_box\n",
    "            else:\n",
    "                self.tracked_bbox = chosen_box\n",
    "        else:\n",
    "            self.tracked_bbox = chosen_box\n",
    "        self.new_target_time = None\n",
    "        \n",
    "        self.x_center_t = (self.tracked_bbox[0] + self.tracked_bbox[2]) / 2\n",
    "        self.y_center_t = (self.tracked_bbox[1] + self.tracked_bbox[3]) / 2\n",
    "        \n",
    "        # Draw a circle at the center of the bounding box\n",
    "        radius = 10  # You can adjust the radius as needed\n",
    "        color = (0, 255, 0)  # Circle color (Green in BGR format)\n",
    "        thickness = 2  # Circle border thickness\n",
    "        cv2.circle(frame, (int(self.x_center_t), int(self.y_center_t)), radius, color, thickness)\n",
    "        cv2.circle(frame, (int(672/2), int(168)), radius, color, thickness)\n",
    "        \n",
    "        #Scaling is necessary for real-time data display.\n",
    "        scale = 0.1 \n",
    "    \n",
    "        # Resize the frame for display\n",
    "        resized_image = cv2.resize(frame, None, fx=scale, fy=scale, interpolation=cv2.INTER_AREA)\n",
    "        display_color.value = bgr8_to_jpeg(resized_image)\n",
    "        \n",
    "        # Process depth image (apply colormap)\n",
    "        depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(camera.depth_image, alpha=0.03), cv2.COLORMAP_JET)\n",
    "        \n",
    "        # Trim the depth image based on tracked bounding box coordinates\n",
    "        depth_colormap[:trim[0], :] = 0\n",
    "        depth_colormap[trim[1]:, :] = 0\n",
    "        depth_colormap[:, :trim[2]] = 0\n",
    "        depth_colormap[:, trim[3]:] = 0\n",
    "    \n",
    "    \n",
    "        # Resize the depth colormap for display\n",
    "        resized_depth_colormap = cv2.resize(depth_colormap, None, fx=scale, fy=scale, interpolation=cv2.INTER_AREA)\n",
    "        display_depth.value = bgr8_to_jpeg(resized_depth_colormap)\n",
    "    \n",
    "        # Now calculate the average depth of the entire depth image (before trimming and resizing)\n",
    "        # Trim depth image to the region of interest\n",
    "        trimmed_depth_image = camera.depth_image[trim[0]:trim[1], trim[2]:trim[3]]\n",
    "    \n",
    "        \n",
    "        # Remove NaN and invalid depth values (if necessary)\n",
    "        trimmed_depth_image_cleaned = np.nan_to_num(trimmed_depth_image, nan=0.0).astype(np.float32)\n",
    "        \n",
    "        # Optionally, replace zero values with NaN (if they represent no depth or invalid data)\n",
    "        trimmed_depth_image_cleaned[trimmed_depth_image_cleaned == 0] = np.nan\n",
    "    \n",
    "        trimmed_depth_image_cleaned[trimmed_depth_image_cleaned<200]=200\n",
    "        trimmed_depth_image_cleaned[trimmed_depth_image_cleaned>3000]=3000\n",
    "        \n",
    "        # Calculate the average depth from the trimmed depth image, and from that calculate a move_speed component with a value between -1 and 1\n",
    "        average_depth = np.nanmean(trimmed_depth_image_cleaned)\n",
    "        self.tracked_depth = average_depth\n",
    "\n",
    "\n",
    "\n",
    "        self.person_pos = (average_depth, self.x_center_t)\n",
    "\n",
    "    @traitlets.observe('person_pos')\n",
    "    def handleMotion(self, change):\n",
    "        depth, x_pos = change['new'][0], change['new'][1]\n",
    "\n",
    "        fwd_speed = (depth - TARGET_DISTANCE) / TARGET_DISTANCE\n",
    "        turn_speed = (x_pos - CENTER_X) / CENTER_X\n",
    "\n",
    "        fwd_speed = clamp(fwd_speed)\n",
    "        turn_speed = clamp(turn_speed)\n",
    "\n",
    "        #Finally, calculate the final motor speeds for left and right motors as components of turn_speed and motor_speed\n",
    "        left_motor_speed = clamp(fwd_speed + turn_speed)\n",
    "        right_motor_speed = clamp(fwd_speed - turn_speed)\n",
    "    \n",
    "        print(f'Speeds: left_motor_speed: {left_motor_speed:.2f} | right_motor_speed: {right_motor_speed:.2f}', end='\\r')\n",
    "    \n",
    "        if (left_motor_speed < 0 and right_motor_speed < 0) or (left_motor_speed > 0 and right_motor_speed > 0) :\n",
    "            left_motor_speed *= 4\n",
    "            right_motor_speed *= 4\n",
    "        \n",
    "        #Move robot\n",
    "        robot.frontLeft(left_motor_speed)\n",
    "        robot.backLeft(left_motor_speed)\n",
    "        robot.frontRight(right_motor_speed)\n",
    "        robot.backRight(right_motor_speed)\n",
    "\n",
    "        \n",
    "        \n",
    "def bgr8_to_jpeg(value):#convert numpy array to jpeg coded data for displaying \n",
    "    return bytes(cv2.imencode('.jpg',value)[1])\n",
    "\n",
    "def clamp(value):\n",
    "    return max(-1, min(1, value))\n",
    "\n",
    "#create a camera object\n",
    "camera = Camera()\n",
    "camera.start() # start capturing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform object detection on live video data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.stop()\n",
    "robot.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
