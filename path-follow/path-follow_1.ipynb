{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "242a9ed3-dd7d-4f84-8b2e-54dd20d9ee49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "#open the txt file and read object categories.\n",
    "f = open(\"paths.txt\", \"r\") \n",
    "labels = []\n",
    "for x in f: # read the txt file line by line\n",
    "    labels.append(x.split(':')[1]) #add it to the list\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d5ceb9-8312-4ccd-852e-a1ed9b3c9a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/robotics/Documents/notebooks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /home/robotics/Documents/notebooks/hub/checkpoints/resnet50-11ad3fa6.pth\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "import os\n",
    "\n",
    "os.environ['TORCH_HOME'] = '/home/robotics/Documents/notebooks' #setting the environment variable\n",
    "model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67be739c-22c0-46f7-aca6-6324137e8418",
   "metadata": {},
   "outputs": [],
   "source": [
    "import traitlets\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pyzed.sl as sl\n",
    "import math\n",
    "import numpy as np\n",
    "import sys\n",
    "import math\n",
    "import threading\n",
    "from traitlets.config.configurable import SingletonConfigurable\n",
    "\n",
    "# Define a Camera class that inherits from SingletonConfigurable\n",
    "class Camera(SingletonConfigurable):\n",
    "    color_value = traitlets.Any() # monitor the color_value variable\n",
    "    def __init__(self):\n",
    "        super(Camera, self).__init__()\n",
    "\n",
    "        self.zed = sl.Camera()\n",
    "        # Create a InitParameters object and set configuration parameters\n",
    "        init_params = sl.InitParameters()\n",
    "        init_params.camera_resolution = sl.RESOLUTION.VGA #VGA(672*376), HD720(1280*720), HD1080 (1920*1080) or ...\n",
    "        init_params.depth_mode = sl.DEPTH_MODE.ULTRA  # Use ULTRA depth mode\n",
    "        init_params.coordinate_units = sl.UNIT.MILLIMETER  # Use meter units (for depth measurements)\n",
    "\n",
    "        # Open the camera\n",
    "        status = self.zed.open(init_params)\n",
    "        if status != sl.ERROR_CODE.SUCCESS: #Ensure the camera has opened succesfully\n",
    "            print(\"Camera Open : \"+repr(status)+\". Exit program.\")\n",
    "            self.zed.close()\n",
    "            exit(1)\n",
    "\n",
    "         # Create and set RuntimeParameters after opening the camera\n",
    "        self.runtime = sl.RuntimeParameters()\n",
    "\n",
    "        #flag to control the thread\n",
    "        self.thread_runnning_flag = False\n",
    "\n",
    "        # Get the height and width\n",
    "        camera_info = self.zed.get_camera_information()\n",
    "        self.width = camera_info.camera_configuration.resolution.width\n",
    "        self.height = camera_info.camera_configuration.resolution.height\n",
    "        self.image = sl.Mat(self.width,self.height,sl.MAT_TYPE.U8_C4, sl.MEM.CPU)\n",
    "        self.depth = sl.Mat(self.width,self.height,sl.MAT_TYPE.F32_C1, sl.MEM.CPU)\n",
    "        self.point_cloud = sl.Mat(self.width,self.height,sl.MAT_TYPE.F32_C4, sl.MEM.CPU) \n",
    "\n",
    "    def _capture_frames(self): #For data capturing only\n",
    "\n",
    "        while(self.thread_runnning_flag==True): #continue until the thread_runnning_flag is set to be False\n",
    "            if self.zed.grab(self.runtime) == sl.ERROR_CODE.SUCCESS:\n",
    "                \n",
    "                # Retrieve Left image\n",
    "                self.zed.retrieve_image(self.image, sl.VIEW.LEFT)\n",
    "                # Retrieve depth map. Depth is aligned on the left image\n",
    "                self.zed.retrieve_measure(self.depth, sl.MEASURE.DEPTH)\n",
    "    \n",
    "                self.color_value = self.image.get_data()\n",
    "                self.color_value= cv2.cvtColor(self.color_value, cv2.COLOR_BGRA2BGR)\n",
    "                self.depth_image = np.asanyarray(self.depth.get_data())\n",
    "                \n",
    "    def start(self): #start the data capture thread\n",
    "        if self.thread_runnning_flag == False: #only process if no thread is running yet\n",
    "            self.thread_runnning_flag=True #flag to control the operation of the _capture_frames function\n",
    "            self.thread = threading.Thread(target=self._capture_frames) #link thread with the function\n",
    "            self.thread.start() #start the thread\n",
    "\n",
    "    def stop(self): #stop the data capture thread\n",
    "        if self.thread_runnning_flag == True:\n",
    "            self.thread_runnning_flag = False #exit the while loop in the _capture_frames\n",
    "            self.thread.join() #wait the exiting of the thread\n",
    "            self.zed.close()\n",
    "\n",
    "def bgr8_to_jpeg(value):#convert numpy array to jpeg coded data for displaying \n",
    "    return bytes(cv2.imencode('.jpg',value)[1])\n",
    "\n",
    "#create a camera object\n",
    "camera = Camera()\n",
    "camera.start() # start capturing the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02d513f9-e032-4002-8720-2a2b1211c55c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63c15dea76814cc3ae2a605bc6b88399",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\x00\\xff\\xdb\\x00Câ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "747\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 48\u001b[0m\n\u001b[1;32m     46\u001b[0m predict_id \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;66;03m#get the label\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(predict_id)\n\u001b[0;32m---> 48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpredict_id\u001b[49m\u001b[43m]\u001b[49m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m prediction time \u001b[39m\u001b[38;5;124m\"\u001b[39m, (cv2\u001b[38;5;241m.\u001b[39mgetTickCount()\u001b[38;5;241m-\u001b[39mt1)\u001b[38;5;241m/\u001b[39mcv2\u001b[38;5;241m.\u001b[39mgetTickFrequency())\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (predict_id) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     50\u001b[0m     robot\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m0.5\u001b[39m) \n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import ipywidgets.widgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "import sys\n",
    "import time\n",
    "import motors\n",
    "\n",
    "#initialize the Robot class\n",
    "robot = motors.MotorsYukon(mecanum=False)\n",
    "\n",
    "#create widgets for the displaying of the image\n",
    "display_color = widgets.Image(format='jpeg', width='45%') #determine the width of the color image\n",
    "display_depth = widgets.Image(format='jpeg', width='45%')  #determine the width of the depth image\n",
    "layout=widgets.Layout(width='100%')\n",
    "\n",
    "sidebyside = widgets.HBox([display_color, display_depth],layout=layout) #horizontal \n",
    "display(sidebyside) #display the widget\n",
    "\n",
    "#callback function, invoked when traitlets detects the changing of the color image\n",
    "def func(change):\n",
    "    #Scaling is necessary for real-time data display.\n",
    "    scale = 0.1\n",
    "    resized_image = cv2.resize(change['new'], None, fx=scale, fy=scale, interpolation=cv2.INTER_AREA)\n",
    "    cv2.circle(resized_image, (int(camera.width*scale//2),int(camera.height*scale//2)), 1, (0, 255, 0))\n",
    "    display_color.value = bgr8_to_jpeg(resized_image)\n",
    "\n",
    "    depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(camera.depth_image , alpha=0.03), cv2.COLORMAP_JET)\n",
    "    resized_depth_colormap=cv2.resize(depth_colormap, None, fx=scale, fy=scale, interpolation=cv2.INTER_AREA)\n",
    "    display_depth.value = bgr8_to_jpeg(resized_depth_colormap)\n",
    "\n",
    "camera.observe(func, names=['color_value'])\n",
    "i=0\n",
    "#the following code will classify the image and move the robot\n",
    "while (i < 100):\n",
    "    t1 = cv2.getTickCount() \n",
    "    imgsized= cv2.resize(camera.color_value,(224,224)) #resize the image\n",
    "    x = cv2.cvtColor(imgsized, cv2.COLOR_BGR2RGB) #convert to RGB as required by the model\n",
    "    x = x.transpose((2, 0, 1)) #swith the image channels\n",
    "    x = torch.from_numpy(x).float() #convert to type float\n",
    "    mean = 255.0 * np.array([0.485, 0.456, 0.406]) #mean value\n",
    "    stdev = 255.0 * np.array([0.229, 0.224, 0.225]) # for the nomalization of the input image\n",
    "    normalize = torchvision.transforms.Normalize(mean, stdev)\n",
    "    x = normalize(x)\n",
    "    x = x.to(device)# send the data to GPU device\n",
    "    x = x[None, ...] # increase the image dimension, the model takes a batch of images and the batch size is 1\n",
    "    output = model(x) #classfy the images\n",
    "    predict_id = output.max(1, keepdim=True)[1].item() #get the label\n",
    "    print(predict_id)\n",
    "    print(labels[predict_id],\" prediction time \", (cv2.getTickCount()-t1)/cv2.getTickFrequency())\n",
    "    if (predict_id) == \"forward\":\n",
    "        robot.forward(0.5) \n",
    "    elif(predict_id == 'left'):\n",
    "        robot.left(0.5) \n",
    "    elif(predict_id == 'right'):\n",
    "        robot.right(0.5) \n",
    "    else:\n",
    "        robot.stop() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba19e06-d905-43f6-af77-96d234291991",
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.stop()\n",
    "robot.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
